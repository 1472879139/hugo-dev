<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习 on 浮生醉清风</title>
        <link>https://1472879139.github.io/hugo-dev/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 机器学习 on 浮生醉清风</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>浮生醉清风</copyright>
        <lastBuildDate>Mon, 22 Sep 2025 14:27:53 +0800</lastBuildDate><atom:link href="https://1472879139.github.io/hugo-dev/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>绪论</title>
        <link>https://1472879139.github.io/hugo-dev/p/%E7%BB%AA%E8%AE%BA/</link>
        <pubDate>Wed, 17 Sep 2025 20:51:56 +0800</pubDate>
        
        <guid>https://1472879139.github.io/hugo-dev/p/%E7%BB%AA%E8%AE%BA/</guid>
        <description>&lt;img src="https://pic3.zhimg.com/v2-dc60ec41d86b0c81703a956889aa89e6_r.jpg" alt="Featured image of post 绪论" /&gt;&lt;h1 id=&#34;绪论&#34;&gt;绪论
&lt;/h1&gt;&lt;p&gt;&lt;img src=&#34;https://tc-new.z.wiki/autoupload/f/y9YdZCt8kR2Z5zWvWUoboNiO_OyvX7mIgxFBfDMDErs/20250922/Fc7i/7680X4320/retouch_2025092001043178.jpg/webp&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;什么是机器学习&#34;&gt;什么是机器学习？
&lt;/h2&gt;&lt;p&gt;机器学习，致力于研究如何通过计算的手段，利用经验来改善系统的自身的性能。“经验”通常以“数据”的形式存在，机器学习的内容主要就是关于在计算机上从数据中产生“模型”的算法。&lt;/p&gt;
&lt;h2 id=&#34;基本术语概念&#34;&gt;基本术语概念
&lt;/h2&gt;&lt;p&gt;这是一组西瓜的数据：&lt;/p&gt;
&lt;p&gt;（色泽=青绿；根蒂=蜷缩；敲声=浊响）&lt;/p&gt;
&lt;p&gt;（色泽=乌黑；根蒂=稍蜷；敲声=沉闷）&lt;/p&gt;
&lt;p&gt;（色泽=浅白；根蒂=硬挺；敲声=清脆）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;数据集&lt;/strong&gt;：对事件或对象描述记录的集合，例如上述三条数据构成数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样本&lt;/strong&gt;：又称为“示例”，数据集的每条记录，关于一个事件或对象的描述，比如其中一条数据，例如（色泽=青绿；根蒂=蜷缩；敲声=浊响）是对西瓜的描述。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;属性&lt;/strong&gt;：反映事件或对象在某方面的表现或性质的事项，例如“色泽”，“根蒂”，“敲声”。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;属性值&lt;/strong&gt;：属性上的取值，例如“青绿”，“乌黑”。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;属性空间&lt;/strong&gt;：属性张成的空间。又称“样本空间”，“输入空间”。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;训练数据&lt;/strong&gt;：训练中使用的使用的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;训练集&lt;/strong&gt;：由训练样本组成的集合称为训练集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;假设&lt;/strong&gt;：学得模型对应关于数据的某种潜在的规律。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;真相&lt;/strong&gt;：这种潜在规律本身。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;标记&lt;/strong&gt;：关于示例结果的信息，例如“好瓜”。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;样例&lt;/strong&gt;：拥有标记信息的示例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;标记空间&lt;/strong&gt;：用$(x_i,y_i)$表示第$i$个样例，其中$y_i \in \mathcal{Y}$是示例$x_i$的标记，$\mathcal{Y}$是所有标记的集合，称为“标记空间”或“输出空间”。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;分类&lt;/strong&gt;：预测的是离散值。例如“好瓜”，“坏瓜”。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;回归&lt;/strong&gt;：预测的是连续值，例如西瓜的成熟度0.95、0.37。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试&lt;/strong&gt;：学得模型后，使用其进行预测的过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;测试样本&lt;/strong&gt;：被预测的样本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;聚类&lt;/strong&gt;：将训练集中的数据分成若干组，每一个组称为一个“簇”。这些自动形成的簇可能对应一些潜在的概念划分。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;监督学习与无监督学习&lt;/strong&gt;：根据训练数据是否拥有标记信息，学习任务大致划分为两大类，“监督学习”和“无监督学习”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;泛化能力&lt;/strong&gt;：学得模型适用于新样本的能力，具有强泛化能力的模型能很好地适用于整个样本空间。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;假设空间&lt;/strong&gt;：学习的过程就是看作在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”的假设，即能够将训练集中的瓜判断正确的假设。假设一旦确定，假设空间及其规模大小就确定，以西瓜例子为例，假设空间由形如“（色泽 = ？） $\wedge$ （根蒂 = ？） $\wedge$ （敲声 = ？）”的可能的取值所形成的假设组成。每个属性（色泽，根蒂，敲声）都有三种具体的取值（如色泽有青绿、乌黑、浅白），再加上通配符“$\ast$”（表示属性取什么值都合适），因此每个属性的实际上有4种可能的取值。这样三种属性的组合总数是$4 \times 4 \times 4 = 64$。但是，假设空间还包括一个极端假设$\sigma$，表示“根本没有好瓜这种概念”（即世界上没有好瓜）。因此，总的假设空间规模为$64 + 1 =65$。&lt;/p&gt;
&lt;p&gt;​	&lt;strong&gt;归纳偏好&lt;/strong&gt;：当多个假设的模型算法做出相同判定时，学习算法的“偏好”起关键作用。机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”。任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上“等效”的假设所迷惑，而无法产生确定的学习结果。&lt;/p&gt;
&lt;h2 id=&#34;没有免费午餐定理&#34;&gt;没有免费午餐定理
&lt;/h2&gt;&lt;p&gt;所有的搜索算法或优化算法或机器学习模型在整体上具有相同的性能。换句话说，不存在一个单一的算法或模型能够在所有问题上都表现得最好。&lt;/p&gt;
&lt;p&gt;假设样本空间$\mathcal{X}$和假设空间$\mathcal{H}$都是离散的。令$P(h \mid X,\mathcal{L}_a)$代表算法$\mathcal{L}_a$基于训练数据$X$产生假设$h$的概率，再令f代表我们希望学习的真实目标函数.$\mathcal{L}_a$的“训练集外误差”，即$\mathcal{L}_a$在训练集之外的所有样本上的误差为：
&lt;/p&gt;
$$
\begin{aligned}
E_{ote}(\mathcal{L}_a | X, f) &amp;= \sum_{h} \sum_{x \in \mathcal{X} - X} P(x) I(h(x) \neq f(x)) P(h | X, \mathcal{L}_a) \\
\end{aligned}
$$&lt;p&gt;
其中$I(\cdot)$是指示函数，若 $\cdot$ 为真则取值为1，否则为0。考虑二分类问题，且真实目标函数可以是任何函数$\mathcal{X} \rightarrow $ {$0,1$}，函数空间为{0，1}$^{|\mathcal{X}|}$ .对所有可能的f按照均匀分布对误差求和，有&lt;/p&gt;
$$
\begin{aligned}
\sum_{f} E_{ote}(\mathcal{L}_a | X, f) &amp;= \sum_{f} \sum_{h} \sum_{x \in \mathcal{X} - X} P(x) I(h(x) \neq f(x)) P(h | X, \mathcal{L}_a) \\
&amp;= \sum_{x \in \mathcal{X} - X} P(x) \sum_{h} P(h | X, \mathcal{L}_a) \sum_{f} I(h(x) \neq f(x)) \\
&amp;= \sum_{x \in \mathcal{X} - X} P(x) \sum_{h} P(h | X, \mathcal{L}_a) \frac{1}{2} 2^{|\mathcal{X}|} \\
&amp;= \frac{1}{2} 2^{|\mathcal{X}|} \sum_{x \in \mathcal{X} - X} P(x) \sum_{h} P(h | X, \mathcal{L}_a) \\
&amp;=2^{|\mathcal{X}| - 1} \sum_{x \in \mathcal{X} - X} P(x) \cdot 1
\end{aligned}
$$&lt;p&gt;
最后公式结果跟学习算法无关，对于任意的两个学习算法$\mathcal{L}_a$ 和 $\mathcal{L}_b$ ，都有
&lt;/p&gt;
$$
\begin{aligned}
\sum_{f} E_{ote}(\mathcal{L}_a \mid X,f) &amp;= \sum_{f} E_{ote}(\mathcal{L}_b \mid X,f)
\end{aligned}
$$&lt;p&gt;
无论学习算法$\mathcal{L}_a$多聪明、学习算法$\mathcal{L}_b$多笨拙，他们的期望性相同，这就是“没有免费午餐”定理。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
